{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Global variable - directory where cfg.json and .dot files generated by our code will be stored \n",
    "part1_output_directory = \"output/part_1/\"\n",
    "part2_output_directory = \"output/part_2/\"\n",
    "\n",
    "# Utility functions taken from TP1\n",
    "def get_json_files(extension, directory):\n",
    "   directory = Path(directory)\n",
    "   return [str(file) for file in directory.rglob(extension)]\n",
    "\n",
    "def create_output_file(filename, directory):\n",
    "    # Check if output directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Check if output file already exists, if so, delete and create new file\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    # Open in \"append\" mode to avoid overwriting the whole file after each modification\n",
    "    return open(directory + filename, \"a\")\n",
    "\n",
    "def close_output_file(file):\n",
    "   file.close()\n",
    "\n",
    "# The following function has been adapted from the following sources:\n",
    "## https://www.tutorialspoint.com/How-to-scan-through-a-directory-recursively-in-Python\n",
    "## https://bito.ai/resources/unzip-gz-file-python-python-explained/\n",
    "## https://stackoverflow.com/questions/42445831/python-3-creating-files-in-relative-directories\n",
    "def extract_zipped_files(source_directory, target_directory):\n",
    "    # Check if target directory exists, if not, create it\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    # Use os.walk to recursively visit the source folder\n",
    "    for current_dir, dir_names, file_names in os.walk(source_directory):\n",
    "        for filename in file_names:\n",
    "            # Find all zipped files\n",
    "            if filename.endswith('.gz'):\n",
    "                # Construct source path from source directory and the filename\n",
    "                source_path = os.path.join(current_dir, filename)\n",
    "\n",
    "                # Construct target path by replacing the source directory with target directory\n",
    "                target_rel_path = os.path.relpath(current_dir, source_directory)\n",
    "                target_dir = os.path.join(target_directory, target_rel_path)\n",
    "                target_path = os.path.join(target_dir, filename[:-3])\n",
    "                \n",
    "                # Check if target directory exists, if not, create it (this is for the target directories within the main directory)\n",
    "                if not os.path.exists(target_dir):\n",
    "                    os.makedirs(target_dir)\n",
    "                \n",
    "                # Open and extract zipped files\n",
    "                with gzip.open(source_path, 'rb') as f_in:\n",
    "                    with open(target_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def __compute_manhatan_distance(vector_a, vector_b):\n",
    "    # Assuming we are receiving a numpy.array type datastructure (better than iterating over each element, substracting then doing the sum)\n",
    "    return numpy.abs(vector_b - vector_a).sum()\n",
    "\n",
    "def is_similar_vector(vector_a, vector_b, similarity_treshold):\n",
    "    return __compute_manhatan_distance(vector_a, vector_b) <= similarity_treshold * numpy.sum(vector_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output file and reader\n",
    "from code_analysis import ASTReader\n",
    "reader = ASTReader ()\n",
    "\n",
    "def find_similar_files(directory, min_nodes, similarity_treshold, enable_identical_skip=False):\n",
    "    # Prepare output file\n",
    "    part1_output_directory = \"output/part_1/\"\n",
    "    # part_1_output_file = create_output_file(\"part_1_output_file.txt\", part1_output_directory)\n",
    "\n",
    "    # Prepare output arrays (we decided agains using a dictionnary for efficiency, a dict with 900 elements seemed to cause the program to be slower)\n",
    "    ## We assumed that since we are populating these arrays at the same time and are not modifying them after, the same index in both arrays should\n",
    "    ## point to the same file.\n",
    "    filename_array = []\n",
    "    vector_array = []\n",
    "\n",
    "    # Retrieve filenames of all ast in the specified directory\n",
    "    astFilenames = get_json_files('*.json', directory)\n",
    "\n",
    "    # Iterate over the filenames array once to visit all ast and retrieve all those with nodes > 100\n",
    "    for filename in astFilenames:\n",
    "        # Load ast in memory\n",
    "        ast = reader.read_ast(filename)\n",
    "\n",
    "        # Only take ASTs with more than 100 nodes into consideration\n",
    "        if len(ast.get_node_ids()) > min_nodes:\n",
    "            vector = ast.vectorize()\n",
    "            filename_array.append(filename)\n",
    "            vector_array.append(vector)  \n",
    "\n",
    "\n",
    "    # Compare all vectors between themselves with using manhattan distance to find the similar ones (where MD < 0.3 in part 1)\n",
    "    for v_a in range(len(vector_array)):\n",
    "        for v_b in range(v_a + 1, len(vector_array)):\n",
    "\n",
    "            # enable_identical_skip = True: compare all files\n",
    "            # enable_identical_skip = False: if identical vectors are found within the same kit, skip the similarity check\n",
    "            if not enable_identical_skip:\n",
    "                # Retrieve the kits' directory name\n",
    "                ## Necessary to check if the identical vectors found are within the same kit\n",
    "                va_kit_dir_name = filename_array[v_a].split('/')[:3]\n",
    "                vb_kit_dir_name = filename_array[v_b].split('/')[:3]\n",
    "\n",
    "                if va_kit_dir_name == vb_kit_dir_name and vector_array[v_a] == vector_array[v_b]:\n",
    "                    continue\n",
    "\n",
    "            if is_similar_vector(vector_array[v_a], vector_array[v_b], similarity_treshold):\n",
    "                print(f\"File {filename_array[v_a]} is similar to {filename_array[v_b]}\")\n",
    "    \n",
    "\n",
    "\n",
    "    # close_output_file(part_1_output_file)\n",
    "            \n",
    "source_dir_part_1 = './ast'\n",
    "target_dir_part_1 = './output/extracted_ast'\n",
    "extract_zipped_files(source_dir_part_1, target_dir_part_1)\n",
    "find_similar_files(target_dir_part_1, 100, 0.3)\n",
    "# find_similar_files(target_dir, 100, 0.3, enable_identical_skip=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
