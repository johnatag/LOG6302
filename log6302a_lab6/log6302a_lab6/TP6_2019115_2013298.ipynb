{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Global variable - directory where cfg.json and .dot files generated by our code will be stored \n",
    "part1_output_directory = \"output/part_1/\"\n",
    "part2_output_directory = \"output/part_2/\"\n",
    "\n",
    "# Utility functions taken from TP1\n",
    "def get_json_files(extension, directory):\n",
    "   directory = Path(directory)\n",
    "   return [str(file) for file in directory.rglob(extension)]\n",
    "\n",
    "def create_output_file(filename, directory):\n",
    "    # Check if output directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Check if output file already exists, if so, delete and create new file\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    # Open in \"append\" mode to avoid overwriting the whole file after each modification\n",
    "    return open(directory + filename, \"a\")\n",
    "\n",
    "def create_fragment_output_file(file_path: str, content: str):\n",
    "    # Check if output directory exists, if not, create it\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Delete the file if it exists\n",
    "    # create the file using append\n",
    "\n",
    "    # Create the file\n",
    "    with open(file_path, \"a\") as file:\n",
    "        # You can optionally write some content to the file here\n",
    "        file.write(content)\n",
    "\n",
    "def close_output_file(file):\n",
    "   file.close()\n",
    "\n",
    "# The following function has been adapted from the following sources:\n",
    "## https://www.tutorialspoint.com/How-to-scan-through-a-directory-recursively-in-Python\n",
    "## https://bito.ai/resources/unzip-gz-file-python-python-explained/\n",
    "## https://stackoverflow.com/questions/42445831/python-3-creating-files-in-relative-directories\n",
    "def extract_zipped_files(source_directory, target_directory):\n",
    "    # Check if target directory exists, if not, create it\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    # Use os.walk to recursively visit the source folder\n",
    "    for current_dir, dir_names, file_names in os.walk(source_directory):\n",
    "        for filename in file_names:\n",
    "            # Find all zipped files\n",
    "            if filename.endswith('.gz'):\n",
    "                # Construct source path from source directory and the filename\n",
    "                source_path = os.path.join(current_dir, filename)\n",
    "\n",
    "                # Construct target path by replacing the source directory with target directory\n",
    "                target_rel_path = os.path.relpath(current_dir, source_directory)\n",
    "                target_dir = os.path.join(target_directory, target_rel_path)\n",
    "                target_path = os.path.join(target_dir, filename[:-3])\n",
    "                \n",
    "                # Check if target directory exists, if not, create it (this is for the target directories within the main directory)\n",
    "                if not os.path.exists(target_dir):\n",
    "                    os.makedirs(target_dir)\n",
    "                \n",
    "                # Open and extract zipped files\n",
    "                with gzip.open(source_path, 'rb') as f_in:\n",
    "                    with open(target_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def __compute_manhatan_distance(vector_a, vector_b):\n",
    "    # Assuming we are receiving a numpy.array type datastructure (better than iterating over each element, substracting then doing the sum)\n",
    "    return numpy.abs(vector_b - vector_a).sum()\n",
    "\n",
    "def is_similar_vector(vector_a, vector_b, similarity_treshold):\n",
    "    return __compute_manhatan_distance(vector_a, vector_b) <= similarity_treshold * numpy.sum(vector_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output file and reader\n",
    "from code_analysis import ASTReader\n",
    "reader = ASTReader ()\n",
    "\n",
    "# Prepare output arrays (we decided agains using a dictionnary for efficiency, a dict with 900 elements seemed to cause the program to be slower)\n",
    "## We assumed that since we are populating these arrays at the same time and are not modifying them after, the same index in both arrays should\n",
    "## point to the same file.\n",
    "filename_array = []\n",
    "vector_array = []\n",
    "\n",
    "def is_identical_kit(first_path: str, second_path: str) -> bool:\n",
    "    first_dir = filename_array[first_path].split('/')[:3]\n",
    "    second_dir = filename_array[second_path].split('/')[:3]\n",
    "    return first_dir == second_dir\n",
    "\n",
    "def find_similar_files(directory, min_nodes, similarity_treshold, enable_identical_skip=False):\n",
    "    # Prepare output file\n",
    "    part1_output_directory = \"output/part_1/\"\n",
    "    # part_1_output_file = create_output_file(\"part_1_output_file.txt\", part1_output_directory)\n",
    "\n",
    "    # Retrieve filenames of all ast in the specified directory\n",
    "    astFilenames = get_json_files('*.json', directory)\n",
    "\n",
    "    # Iterate over the filenames array once to visit all ast and retrieve all those with nodes > 100\n",
    "    for filename in astFilenames:\n",
    "        # Load ast in memory\n",
    "        ast = reader.read_ast(filename)\n",
    "\n",
    "        # Only take ASTs with more than 100 nodes into consideration\n",
    "        if len(ast.get_node_ids()) > min_nodes:\n",
    "            vector = ast.vectorize()\n",
    "            filename_array.append(filename)\n",
    "            vector_array.append(vector)  \n",
    "\n",
    "\n",
    "    # Compare all vectors between themselves with using manhattan distance to find the similar ones (where MD < 0.3 in part 1)\n",
    "    for v_a in range(len(vector_array)):\n",
    "        for v_b in range(v_a + 1, len(vector_array)):\n",
    "\n",
    "            # enable_identical_skip = True: compare all files\n",
    "            # enable_identical_skip = False: if identical vectors are found within the same kit, skip the similarity check\n",
    "            if enable_identical_skip and is_identical_kit(v_a, v_b):\n",
    "                continue\n",
    "\n",
    "            if is_similar_vector(vector_array[v_a], vector_array[v_b], similarity_treshold):\n",
    "                print(f\"File {filename_array[v_a]} is similar to {filename_array[v_b]}\")\n",
    "\n",
    "            # Compare all + compute manhattan distance with threshhold\n",
    "            # Ignore duplicated inside the same kit\n",
    "            # \n",
    "    \n",
    "\n",
    "\n",
    "    # close_output_file(part_1_output_file)\n",
    "            \n",
    "# source_dir_part_1 = './ast'\n",
    "# target_dir_part_1 = './output/extracted_ast'\n",
    "# extract_zipped_files(source_dir_part_1, target_dir_part_1)\n",
    "# find_similar_files(target_dir_part_1, 100, 0.3)\n",
    "# find_similar_files(target_dir, 100, 0.3, enable_identical_skip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from code_analysis import CFG, ASTReader, CFGReader, Graph, AST_fragmentation\n",
    "from graphviz import Source\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel\n",
    "\n",
    "part2_output_directory = \"./output/extracted_fragments/\"\n",
    "\n",
    "def output_fragments(source_directory, target_directory):\n",
    "    # Check if target directory exists, if not, create it\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    # Use os.walk to recursively visit the source folder\n",
    "    for current_dir, dir_names, file_names in os.walk(source_directory):\n",
    "        for filename in file_names:\n",
    "            # Find all zipped files\n",
    "            if filename.endswith('.ast.json'):\n",
    "                # Construct source path from source directory and the filename\n",
    "                source_path = os.path.join(current_dir, filename)\n",
    "\n",
    "                # Construct target path by replacing the source directory with target directory\n",
    "                target_rel_path = os.path.relpath(current_dir, source_directory)\n",
    "                target_dir = os.path.join(target_directory, target_rel_path)\n",
    "                target_path = os.path.join(target_dir, filename[:-3])\n",
    "                \n",
    "                # Check if target directory exists, if not, create it (this is for the target directories within the main directory)\n",
    "                if not os.path.exists(target_dir):\n",
    "                    os.makedirs(target_dir)\n",
    "                \n",
    "                # Write the extracted fragments in their own ast.json file\n",
    "                ast = ASTReader().read_ast(source_path)\n",
    "                fragments = AST_fragmentation(ast)\n",
    "\n",
    "                for idx, fragment in enumerate(fragments):\n",
    "                    output_filename = f\"{filename}_{idx}.fragment.json\"\n",
    "                    output_filepath = os.path.join(target_dir, output_filename)\n",
    "                    ast_copy = copy.deepcopy(ast)\n",
    "                    ast_copy.set_root(fragment)\n",
    "                    create_fragment_output_file(output_filepath, ast_copy.to_json())\n",
    "\n",
    "#output_fragments(\"./output/extracted_ast/\", target_directory=part2_output_directory)\n",
    "source_dir_part_2 = './output/extracted_fragments'\n",
    "find_similar_files(source_dir_part_2, 10, 0.1, enable_identical_skip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer pour chaque kit un vecteur le representant, en sommant (par colonne) tout les fragments qui composent ce kit.\n",
    "def get_parametric_kits(dir_path: str):\n",
    "    # Get all files and directories in the specified directory\n",
    "    all_entries = os.listdir(dir_path)\n",
    "    ast_reader = ASTReader()\n",
    "    # Filter out only directories\n",
    "    directories = [entry for entry in all_entries if os.path.isdir(os.path.join(dir_path, entry))]\n",
    "\n",
    "    for directory in directories:\n",
    "        print(directory)\n",
    "        # List to store the files\n",
    "        \n",
    "\n",
    "        # Walk through the directory\n",
    "        for root, dirs, files in os.walk(f\"{dir_path}/{directory}\"):\n",
    "            for file in files:\n",
    "                if file.endswith('.fragment.json'):\n",
    "                    fragment_file_path = os.path.join(root, file)\n",
    "\n",
    "                    ast = ast_reader.read_ast(fragment_file_path)\n",
    "                    vector = ast.vectorize() # Represents the vector of a fragment\n",
    "\n",
    "\n",
    "\n",
    "        # Print the list of .fragment.json files\n",
    "        for file in fragment_files:\n",
    "            print(file)\n",
    "\n",
    "\n",
    "get_parametric_kits(source_dir_part_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
